---
title: Setting up a highly-available Teleport cluster using AWS & EKS
description: How to install and configure a highly-available Gravitational Teleport cluster using Amazon Web Services and Elastic Kubernetes Service
---

In this guide, we'll go through how to set up a highly-availabie Teleport cluster with multiple replicas in Kubernetes
using Teleport Helm charts and AWS products (DynamoDB and S3).

This guide assumes that you've finished the [Getting Started with the Teleport Helm chart repository guide](./getting-started.mdx) and have Helm configured successfully.

If not, please finish that guide first before starting this one!

### Setting up an HA Teleport cluster using Helm

In `aws` mode, the `teleport-cluster` Helm chart uses AWS DynamoDB for storing its backend database and audit logs. The highly-available
shared storage of DynamoDB means that multiple Teleport pods can use the same backing database, enabling a Teleport cluster to scale
efficiently with load.

The chart also uses DynamoDB to hold Teleport's audit log, keeping track of actions performed within your cluster.

Teleport's session recordings will be stored in an S3 bucket, providing functional, efficient storage for any number of users.

### Mandatory settings

You'll need five pieces of information to configure `aws` mode:

| Key | Description | Example |
| - | - | - |
| `clusterName` | Name of your Teleport cluster. We recommend using the fully-qualified domain name you'll use for external access to your cluster | `teleport.example.com` |
| `aws.region` | AWS region to use for Teleport's DynamoDB tables | `us-west-2` |
| `aws.backendTable` | Name of the DynamoDB table to use for Teleport's backend database | `teleport-helm-backend` |
| `aws.auditLogTable` | Name of the DynamoDB table to use for Teleport's audit event log (this **must** be different to `backendTable` due to differing schemas) | `teleport-helm-events` |
| `aws.sessionRecordingBucket` | Name of the S3 bucket to use for Teleport's session recordings | `teleport-helm-sessions` |

<Admonition type="note">
  You cannot change the `clusterName` after the cluster is configured, so make sure you choose wisely. It's usually a good idea to use the
  fully-qualified domain name that you'll use for external access to your Teleport cluster.
</Admonition>

#### High availability

To make your cluster highly available, set the `highAvailability.replicaCount` parameter to the number of replicas you would like.

In this guide, we'll be configuring 2 replicas. This can be changed after installation [using `helm upgrade` as detailed below](#making-changes-to-the-cluster-after-deployment).

See the [high availability section of our `teleport-cluster` chart reference](./teleport-cluster-reference.mdx#highavailability) for more details.

### AWS IAM configuration

For Teleport to be able to create the DynamoDB tables, indexes and the S3 storage bucket it needs,
you'll need to configure AWS IAM polciies to allow access.

<Admonition type="note">
  These IAM policies should be added to your AWS account, then granted to the instance role associated with the
  EKS nodegroups which are running your Kubernetes nodes.
</Admonition>

#### DynamoDB IAM policy

You'll need to replace these values in the policy example below:

| Placeholder value | Replace with | Example |
| - | - | - |
| `<region>` | AWS region | `us-west-2` |
| `<account-id>` | AWS account ID | `1234567890` |
| `<backendTable-name>` | The value from `aws.backendTable` above | `teleport-helm-backend` |
| `<auditLogTable-name>` | The value from `aws.auditLogTable` above | `teleport-helm-events` |

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "ClusterStateStorage",
            "Effect": "Allow",
            "Action": [
                "dynamodb:BatchWriteItem",
                "dynamodb:UpdateTimeToLive",
                "dynamodb:PutItem",
                "dynamodb:DeleteItem",
                "dynamodb:Scan",
                "dynamodb:Query",
                "dynamodb:DescribeStream",
                "dynamodb:UpdateItem",
                "dynamodb:DescribeTimeToLive",
                "dynamodb:CreateTable",
                "dynamodb:DescribeTable",
                "dynamodb:GetShardIterator",
                "dynamodb:GetItem",
                "dynamodb:UpdateTable",
                "dynamodb:GetRecords"
            ],
            "Resource": [
                "arn:aws:dynamodb:<region>:<account-id>:table/<backendTable-name>",
                "arn:aws:dynamodb:<region>:<account-id>:table/<backendTable-name>/stream/*"
            ]
        },
        {
            "Sid": "ClusterEventsStorage",
            "Effect": "Allow",
            "Action": [
                "dynamodb:CreateTable",
                "dynamodb:BatchWriteItem",
                "dynamodb:UpdateTimeToLive",
                "dynamodb:PutItem",
                "dynamodb:DescribeTable",
                "dynamodb:DeleteItem",
                "dynamodb:GetItem",
                "dynamodb:Scan",
                "dynamodb:Query",
                "dynamodb:UpdateItem",
                "dynamodb:DescribeTimeToLive",
                "dynamodb:UpdateTable"
            ],
            "Resource": [
                "arn:aws:dynamodb:<region>:<account-id>:table/<auditLogTable-name>",
                "arn:aws:dynamodb:<region>:<account-id>:table/<auditLogTable-name>/index/*"
            ]
        }
    ]
}
```

#### S3 IAM policy

You'll need to replace these values in the policy example below:

| Placeholder value | Replace with | Example |
| - | - | - |
| `<sessionRecordingBucket-name>` | The value from `aws.sessionRecordingBucket` above | `teleport-helm-sessions` |

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "ClusterSessionsStorage",
            "Effect": "Allow",
            "Action": [
                "s3:PutEncryptionConfiguration",
                "s3:PutObject",
                "s3:GetObject",
                "s3:GetEncryptionConfiguration",
                "s3:GetObjectRetention",
                "s3:ListBucketVersions",
                "s3:CreateBucket",
                "s3:ListBucket",
                "s3:GetBucketVersioning",
                "s3:PutBucketVersioning",
                "s3:GetObjectVersion"
            ],
            "Resource": [
                "arn:aws:s3:::<sessionRecordingBucket-name>/*",
                "arn:aws:s3:::<sessionRecordingBucket-name>"
            ]
        }
    ]
}
```

### Choose how to configure the cluster

There are two different ways to configure the `teleport-cluster` Helm chart to use `aws` mode - using a `values.yaml` file or using `--set`
on the command line.

We recommend using a `values.yaml` file as it can be easily kept in source control.

The `--set` CLI method is more appropriate for quick test deployments.

<Tabs>
  <TabItem label="Using values.yaml">
  Create an `aws-values.yaml` file and write the values you've chosen above to it:

  ```yaml
  clusterName: teleport.example.com
  chartMode: aws
  aws:
    region: us-west-2
    backendTable: teleport-helm-backend
    auditLogTable: teleport-helm-events
    sessionRecordingBucket: teleport-helm-sessions
  highAvailability:
    replicaCount: 2
  ```

  Install the chart with the values from your `aws-values.yaml` file using this command:

  ```shell
  $ helm install teleport teleport/teleport-cluster \
    --create-namespace \
    --namespace teleport \
    -f aws-values.yaml
  ```

  </TabItem>
  <TabItem label="Using --set via CLI">
  Install the chart using this command, replacing the placeholders with the values you've chosen above:

  ```shell
  $ helm install teleport teleport/teleport-cluster \
    --create-namespace \
    --namespace teleport \
    --set clusterName=teleport.example.com \
    --set chartMode=aws \
    --set aws.region=us-west-2 \
    --set aws.backendTable=teleport-helm-backend \
    --set aws.auditLogTable=teleport-helm-events \
    --set aws.sessionRecordingBucket=teleport-helm-sessions \
    --set highAvailability.replicaCount=2
  ```
  </TabItem>
</Tabs>

<Admonition type="tip">
  It will help if you have access to the DNS provider which hosts `example.com` so you can add a `teleport.example.com` record
  and point it to the external IP or hostname of the Kubernetes load balancer.

  If you're using AWS Route53, you can do this by using [`aws route53 change-resource-record-sets`](https://docs.aws.amazon.com/cli/latest/reference/route53/change-resource-record-sets.html)
  on the command line, or using the Route53 UI.

  You can also consider using other Kubernetes-based services like [external-dns](https://github.com/kubernetes-sigs/external-dns).

  For other DNS providers, consult their documentation.
</Admonition>

Once the chart is installed, you can use `kubectl` commands to view the deployment:

```shell
$ kubectl --namespace teleport get all
NAME                            READY   STATUS    RESTARTS   AGE
pod/teleport-5cf46ddf5f-dzh65   1/1     Running   0          21s
pod/teleport-5cf46ddf5f-mpghq   1/1     Running   0          21s

NAME               TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)                                                      AGE
service/teleport   LoadBalancer   10.100.37.171   a232d92df01f940339adea0e645d88bb-1576732600.us-east-1.elb.amazonaws.com   443:30821/TCP,3023:30801/TCP,3026:32612/TCP,3024:31253/TCP   21s

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/teleport   2/2     2            2           21s

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/teleport-5cf46ddf5f   2         2         2       21s
```

At this point, you'll still need to create a user to be able to log into Teleport. This needs to be done on the Teleport auth server,
so we can run the command using `kubectl`:

```shell
$ kubectl --namespace teleport exec deploy/teleport -- tctl users add test --roles=access,editor
User "test" has been created but requires a password. Share this URL with the user to complete user setup, link is valid for 1h:
https://teleport.example.com:443/web/invite/91cfbd08bc89122275006e48b516cc68

NOTE: Make sure teleport.example.com:443 points at a Teleport proxy which users can access.
```

<Admonition type="note">
  If you didn't set up DNS for your hostname earlier, remember to replace `teleport.example.com` with the external IP or hostname of the Kubernetes load balancer.

  ```shell
  $ kubectl --namespace teleport get service/teleport -o jsonpath='{.status.loadBalancer.ingress[*].hostname}'
  a232d92df01f940339adea0e645d88bb-1576732600.us-east-1.elb.amazonaws.com
  ```

  In this instance, you would load https://a232d92df01f940339adea0e645d88bb-1576732600.us-east-1.elb.amazonaws.com/web/invite/91cfbd08bc89122275006e48b516cc68
  instead to create the Teleport user.

  You may need to accept insecure warnings in your browser to view the page successfully.
</Admonition>

<Admonition type="warning">
  Using a Kubernetes-issued load balancer IP or hostname is OK for testing, but is not viable when running a production Teleport cluster
  as the Subject Alternative Name on any public-facing certificate will be expected to match the cluster's configured public address (which is
  the same as the configured `clusterName` when using the `teleport-cluster` chart). For security, all Teleport services will validate this when
  communicating with the cluster.

  You must configure DNS properly using methods described above for production workloads.
</Admonition>

Load the user creation link to create a password and set up 2-factor authentication for the Teleport user via the web UI.

You can follow our [Getting Started with Teleport guide](../../getting-started.mdx#step-2-create-a-teleport-user-and-set-up-2-factor-authentication) to finish setting up your
Teleport cluster.

### Making changes to the cluster after deployment

To make changes to your Teleport cluster after deployment, you can use `helm upgrade`:

<Tabs>
  <TabItem label="Using values.yaml">
  Edit your `aws-values.yaml` file from above and make the appropriate changes.

  Upgrade the deployment with the values from your `aws-values.yaml` file using this command:

  ```shell
  $ helm upgrade teleport teleport/teleport-cluster \
    --namespace teleport \
    -f aws-values.yaml
  ```

  </TabItem>
  <TabItem label="Using --set via CLI">
  Run this command, editing your command line parameters as appropriate:

  ```shell
  $ helm upgrade teleport teleport/teleport-cluster \
    --namespace teleport \
    --set highAvailability.replicaCount=3
  ```
  </TabItem>
</Tabs>

<Admonition type="note">
  To change `chartMode`, `clusterName` or any `aws` settings, you must first uninstall the existing chart and then install
  a new version with the appropriate values.
</Admonition>

### Uninstalling the Helm chart

To uninstall the `teleport-cluster` chart, use `helm uninstall <release-name>`. For example:

```shell
$ helm --namespace teleport uninstall teleport
```
